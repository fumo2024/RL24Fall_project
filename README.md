# 强化学习课程期末项目
欢迎来到笨喵的强化学习课程期末项目仓库！本仓库包含了笨喵在课程期间完成的最终项目代码。

## 关于我们
笨喵来自中国以外的一所三流大学，虽然笨喵的学校可能不像一些顶尖学府那样广为人知， 但是所有成员对于项目目标齐心尽力，争取以高效率的要求完成课程目标。 希望所有参与者与旁观者轻松愉快，希望项目开发顺利，希望抽卡不歪，希望能找到钱多事少离家近的好工作， 希望学长不要来找我秀他有多受欢迎，希望不再有学弟来找我问学妹的联系方式，希望明年TGA还在世(嘘)， 希望能有大气不挑剔的甲方， 希望不洗漱也不会散发私人气息，希望明年50系显卡不会让40系崩盘，希望明年本喵还在。 这个项目是喵们团队成员共同智慧与汗水的结晶，如果您希望联系我们，请...你真的要联系我们。

## 代码来源说明
值得注意的是，为了辅助学习和提高效率，笨喵在开发过程中使用了各种AI工具来生成代码。这些工具帮助笨喵更好地理解了强化学习的概念，并加速了项目的进展。

## 使用许可
笨喵并不知道什么使用许可，所以笨喵随机选择了一个使用许可，本项目使用[MIT](./LICENSE.TXT)许可。

## 社区准则
笨喵相信一个健康的社区环境对于所有参与者都是至关重要的。因此，我们鼓励大家在使用和贡献时秉持尊重、理解和合作的精神。让我们一起构建一个积极向上的开源社区吧！Ciallo～(∠・ω< )⌒★

感谢您对笨喵工作的支持与理解。

## 项目简介
### 项目背景介绍
深度强化学习是一种通过环境交互进行学习的方法。它类似于人类的学习过程，根据外界反馈进行学习。因此，它在物理世界得到广泛应用，例如机器人、互联网、自动驾驶等。 

本课程以引导学生将强化学习的相关理论应用到科研中为目标，该目标要求学生同时掌握强化学习的算法理论以及相关算法的实现与应用。本学期的课程已经介绍了许多经典的深度强化学习理论，包括深度神经网络、多臂老虎机、马尔科夫决策过程、动态规划法、蒙特卡洛法、时分求解法等。但是，仅仅通过课堂上的理论学习和课下作业是无法满足本课程的目标的。因此本课程设计了课程大作业，意图通过给定的应用场景锻炼学生的应用与实践能力，从而为后续研究工作打下坚实的基础。 

考虑到课程中的学生的研究方向不同，若针对某特定领域可能会增加学生理解应用场景的时间成本，这与本课程的目标是背道而驰的。因此本课程设计一种几乎所有人都耳熟能详的无禁手五子棋(Gomoku)作为应用场景。无禁手五子棋是一种两人对弈的纯策略型棋类游戏，通常双方分别使用黑白两色的棋子，下在棋盘直线与横线的交叉点上，先形成5子连线者获胜。 

结合本次大作业的应用实例，学习强化学习相关方法的使用，从而使学生具备使用强化学习工具开展研究或应用的良好知识基础。在实现大作业的过程中，学生往往需要阅读大量的前沿论文，这将加深学生对强化学习基础理论的理解。

### 游戏规则 
对弈双方分别使用黑白两色的棋子，下在棋盘直线与横线的交叉点上，先形成5子连线者获胜
### 项目具体介绍
我们没有考虑使用传统强化学习得方法来完成这次的项目，相反的，我们考虑在agent中加入时下最热门的LLM！虽然我们也可以加入更加热门的MLLM，但是经过课程要求的一直否决后，我们小组内部在方案上达成了一致！

我们考虑利用LLM完成对弈策略的实现与优化，包括但不限于调用LLM获取对弈指导，实现基于LLM的RL算法等。
### LLM调用说明
我们使用deepseek的api接口:https://api-docs.deepseek.com/zh-cn。使用说明如下：
1. 安装pip install openai==0.28 
2. 调用方式参考官方文档https://api-docs.deepseek.com/zh-cn/，可能根据 `openai`库版本有所差异，善用搜索 (实际上我们没有在此过多尝试，有可能以此来做各个LLM之间的benchmark吗)

```Python 
# ！pip install openai==0.28 
openai.api_key = API_KEY 
openai.api_base = BASE_URL 
response = openai.ChatCompletion.create( 
  model="deepseek-chat", 
  messages=[ 
    {"role": "system", "content": "You are a helpful assistant"}, 
    {"role": "user", "content": "Hello"}, 
    ],
    stream=False 
) 
print(response.choices[0].message.content)
```
### 代码结构说明
1、为了保证大家的apikey安全，笨喵将apikey单独放在了`apikey.json`文件中，并且加入了`.gitignore`来防呆。

2、`run_game.sh`为一个简单的shell脚本，在命令行键入`./run_game.sh`以运行。它会将日志文件存放在`run`文件夹下，但是笨喵发现没人喜欢用这东西，所以就不维护了，它有可能出错。

3、`simulator`文件夹下存放了一个用来模拟五子棋对局的网页工具，在命令行键入`python3 app.py`以运行。目前不支持交互下棋，将准备好的对局信息放`assets`目录下，就可以可视化对局信息了。

4、各个branch目前的情况
  - observe1，最naive的实现，prompt采取的助教一套，没有在程序上限制落子合法性
  - observe2, 修改了prompt以及在程序上添加了合法性限制，采用的prompt提示LLM输出固定结构语句，再用正则匹配提取的模式
  - observe2R，在observe2的基础上，添加了多轮对话的re_ask机制
  - observe3, 用来实验evaluate效果的实验性分支
  - version0.9, 存档之前的开发分支
  - version1, 实现了比较好的代码结构的一个版本
  - main, 还在开发的主要分支

5、version1代码运行说明，可以在play_game.py中修改对局的两个智能体信息，从而实现人类棋手、LLM或者弱AI之间的任意组合，终端运行命令如下，选项可选打开
```shell
python3 play_game.py --DEBUG --output filename
```

6、version1代码文件说明
  - simulator文件夹，存放了一个网页端可视化小程序，可以供录视频使用，使用的输入由主程序--output指令输出，可以暂停、自动/手动播放、调节速度。
  - agent.py，存放了各个agent的实现，东西有点多
  - apikey.json，github上没有，但是运行LLMagent需要创建并输入deepseek上的，其它平台的apikey目前还没适用
  - ChessBoard.py，存放Board类和Game类
  - mcts_pure.py，从其它地方找来的简单MCTS实现
  - play_game.py，封装的主程序，主要实例化一个Game类，来管理对局过程
  - run_game.sh，一个展示如何使用play_game.py的终端脚本程序